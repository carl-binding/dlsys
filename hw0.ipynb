{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d25dbfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "import numpy as np\n",
    "import gzip\n",
    "import math\n",
    "import numdifftools as nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2b1c8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_mnist(image_filename, label_filename, num_samples=60000):\n",
    "    \"\"\" Read an images and labels file in MNIST format.  See this page:\n",
    "    http://yann.lecun.com/exdb/mnist/ for a description of the file format.\n",
    "\n",
    "    Args:\n",
    "        image_filename (str): name of gzipped images file in MNIST format\n",
    "        label_filename (str): name of gzipped labels file in MNIST format\n",
    "\n",
    "    Returns:\n",
    "        Tuple (X,y):\n",
    "            X (numpy.ndarray[np.float32]): 2D numpy array containing the loaded \n",
    "                data.  The dimensionality of the data should be \n",
    "                (num_examples x input_dim) where 'input_dim' is the full \n",
    "                dimension of the data, e.g., since MNIST images are 28x28, it \n",
    "                will be 784.  Values should be of type np.float32, and the data \n",
    "                should be normalized to have a minimum value of 0.0 and a \n",
    "                maximum value of 1.0 (i.e., scale original values of 0 to 0.0 \n",
    "                and 255 to 1.0).\n",
    "\n",
    "            y (numpy.ndarray[dtype=np.uint8]): 1D numpy array containing the\n",
    "                labels of the examples.  Values should be of type np.uint8 and\n",
    "                for MNIST will contain the values 0-9.\n",
    "    \"\"\"\n",
    "    ### BEGIN YOUR CODE\n",
    "    \n",
    "    import gzip\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import struct\n",
    "\n",
    "    from urllib.request import urlretrieve \n",
    "\n",
    "    def load_data(src, num_samples):\n",
    "        print(\"Downloading \" + src)\n",
    "        ## create a temporary file\n",
    "        gzfname, h = urlretrieve(src, \"./delete.me\")\n",
    "        print(\"Done.\")\n",
    "        ## unpack the data\n",
    "        try:\n",
    "            with gzip.open(gzfname) as gz:\n",
    "                n = struct.unpack(\"I\", gz.read(4))\n",
    "                # Read magic number.\n",
    "                if n[0] != 0x3080000:\n",
    "                    raise Exception(\"Invalid file: unexpected magic number.\")\n",
    "                # Read number of entries.\n",
    "                n = struct.unpack(\">I\", gz.read(4))[0]\n",
    "                if n != num_samples:\n",
    "                    raise Exception(\n",
    "                        \"Invalid file: expected {0} entries.\".format(num_samples)\n",
    "                    )\n",
    "                ## number of rows & columns\n",
    "                crow = struct.unpack(\">I\", gz.read(4))[0]\n",
    "                ccol = struct.unpack(\">I\", gz.read(4))[0]\n",
    "                if crow != 28 or ccol != 28:\n",
    "                    raise Exception(\n",
    "                        \"Invalid file: expected 28 rows/cols per image.\"\n",
    "                    )\n",
    "                # Read data.\n",
    "                res = np.frombuffer( gz.read(num_samples * crow * ccol), dtype=np.uint8)\n",
    "        finally:\n",
    "            ## delete the temp file\n",
    "            os.remove(gzfname)\n",
    "        ## reshape to (num_samples, crow * ccol) and normalize to [0.0..1.0]\n",
    "        ## uint8 range is [0..255]...\n",
    "        res = res.reshape((num_samples, crow * ccol)) / 255.0\n",
    "        ## make sure it's float32 and not float64...\n",
    "        return res.astype( 'float32')\n",
    "\n",
    "\n",
    "    def load_labels(src, num_samples):\n",
    "        print(\"Downloading \" + src)\n",
    "        gzfname, h = urlretrieve(src, \"./delete.me\")\n",
    "        print(\"Done.\")\n",
    "        try:\n",
    "            with gzip.open(gzfname) as gz:\n",
    "                n = struct.unpack(\"I\", gz.read(4))\n",
    "                # Read magic number.\n",
    "                if n[0] != 0x1080000:\n",
    "                    raise Exception(\"Invalid file: unexpected magic number.\")\n",
    "                # Read number of entries.\n",
    "                n = struct.unpack(\">I\", gz.read(4))\n",
    "                if n[0] != num_samples:\n",
    "                    raise Exception(\n",
    "                        \"Invalid file: expected {0} rows.\".format(num_samples)\n",
    "                    )\n",
    "                # Read labels.\n",
    "                res = np.frombuffer(gz.read(num_samples), dtype=np.uint8)\n",
    "        finally:\n",
    "            os.remove(gzfname)\n",
    "        return res.reshape((num_samples))\n",
    "\n",
    "\n",
    "    def try_download(data_source, label_source, num_samples):\n",
    "        data = load_data(data_source, num_samples)\n",
    "        labels = load_labels(label_source, num_samples)\n",
    "        return data, labels\n",
    "    \n",
    "\n",
    "    ## server = 'https://yann.lecun.com/exdb/mnist/'\n",
    "    server = 'https://raw.githubusercontent.com/fgnt/mnist/master/'\n",
    "    \n",
    "    # URLs for the train image and label data\n",
    "    url_train_image = server + image_filename\n",
    "    url_train_labels = server + label_filename\n",
    "    ## num_train_samples = 60000\n",
    "\n",
    "    print(\"Downloading train data: \" + url_train_image + \", \" + url_train_labels)\n",
    "    train_features, train_labels = try_download(url_train_image, url_train_labels, num_samples)\n",
    "    \n",
    "    print( \"Downloading done...\")\n",
    "    \n",
    "    return ( train_features, train_labels)\n",
    "  \n",
    "    ### END YOUR CODE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc878582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_parse_mnist():\n",
    "    ## https://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
    "    ## https://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
    "    X,y = parse_mnist(\"train-images-idx3-ubyte.gz\",\n",
    "                      \"train-labels-idx1-ubyte.gz\")\n",
    "    assert X.dtype == np.float32\n",
    "    assert y.dtype == np.uint8\n",
    "    assert X.shape == (60000,784)\n",
    "    assert y.shape == (60000,)\n",
    "    \n",
    "    \n",
    "    np.testing.assert_allclose(np.linalg.norm(X[:10]), 27.892084)\n",
    "    np.testing.assert_allclose(np.linalg.norm(X[:1000]), 293.0717,\n",
    "        err_msg=\"\"\"If you failed this test but not the previous one,\n",
    "        you are probably normalizing incorrectly. You should normalize\n",
    "        w.r.t. the whole dataset, _not_ individual images.\"\"\", rtol=1e-6)\n",
    "    np.testing.assert_equal(y[:10], [5, 0, 4, 1, 9, 2, 1, 3, 1, 4])\n",
    "    print( \"test successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "315dffe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train data: https://raw.githubusercontent.com/fgnt/mnist/master/train-images-idx3-ubyte.gz, https://raw.githubusercontent.com/fgnt/mnist/master/train-labels-idx1-ubyte.gz\n",
      "Downloading https://raw.githubusercontent.com/fgnt/mnist/master/train-images-idx3-ubyte.gz\n",
      "Done.\n",
      "Downloading https://raw.githubusercontent.com/fgnt/mnist/master/train-labels-idx1-ubyte.gz\n",
      "Done.\n",
      "Downloading done...\n",
      "test successful\n"
     ]
    }
   ],
   "source": [
    "test_parse_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b6886d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss(Z, y):\n",
    "    \"\"\" Return softmax loss.  Note that for the purposes of this assignment,\n",
    "    you don't need to worry about \"nicely\" scaling the numerical properties\n",
    "    of the log-sum-exp computation, but can just compute this directly.\n",
    "\n",
    "    Args:\n",
    "        Z (np.ndarray[np.float32]): 2D numpy array of shape\n",
    "            (batch_size, num_classes), containing the logit predictions for\n",
    "            each class.\n",
    "        y (np.ndarray[np.uint8]): 1D numpy array of shape (batch_size, )\n",
    "            containing the true label of each example.\n",
    "\n",
    "    Returns:\n",
    "        Average softmax loss over the sample.\n",
    "    \"\"\"\n",
    "    ### BEGIN YOUR CODE\n",
    "    ## nbr of rows in Z == length of y?\n",
    "    ## print( \"length of y = \" + str(len( y)))\n",
    "    \n",
    "    assert len(y) == np.shape( Z)[0]\n",
    "   \n",
    "    Z_exp = np.exp( Z)\n",
    "    \n",
    "    ## sum along rows\n",
    "    sum_rows_Z = np.sum( Z_exp, axis=1)\n",
    "    \n",
    "    ## print( \"sum along rows done...\")\n",
    "    \n",
    "    log_sum_rows_Z = np.log( sum_rows_Z)\n",
    "    \n",
    "    ## print( \"log of sums done...\")\n",
    "    \n",
    "    ## extract the probs for the correct label y[i] from i-th row in Z\n",
    "    ## this runs forever, stalling python....\n",
    "    ## ZZ = - Z [:,y]\n",
    "\n",
    "    ## thanks to ChatGPT, this works... we create an index array of the same shape as the \"other\" index array y...\n",
    "    row_index = np.arange( Z.shape[0]) ## nbr of rows\n",
    "    ZZ = Z[row_index, y]\n",
    "\n",
    "    '''\n",
    "    ## YUCK, a horrible loop...\n",
    "    ZZ = np.zeros( len( y))\n",
    "    for i in range( len( y)):\n",
    "        ZZ[i] = Z[i, y[i]]\n",
    "    '''\n",
    "    \n",
    "    ZZ = - ZZ + log_sum_rows_Z\n",
    "    \n",
    "    ## print( \"cross entropy done...\")\n",
    "    \n",
    "    ## len(y) == batch_size\n",
    "    ZZ = np.sum( ZZ)/len( y)\n",
    "   \n",
    "    return ZZ\n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1878da89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_softmax_loss():\n",
    "    \n",
    "    X,y = parse_mnist(\"train-images-idx3-ubyte.gz\",\n",
    "                      \"train-labels-idx1-ubyte.gz\")\n",
    "    np.random.seed(0)\n",
    "\n",
    "    Z = np.zeros((y.shape[0], 10))\n",
    "    \n",
    "    np.testing.assert_allclose(softmax_loss(Z,y), 2.3025850)\n",
    "    \n",
    "    Z = np.random.randn(y.shape[0], 10)\n",
    "    np.testing.assert_allclose(softmax_loss(Z,y), 2.7291998)\n",
    "    \n",
    "    print( \"test sucessful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7d17d209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train data: https://raw.githubusercontent.com/fgnt/mnist/master/train-images-idx3-ubyte.gz, https://raw.githubusercontent.com/fgnt/mnist/master/train-labels-idx1-ubyte.gz\n",
      "Downloading https://raw.githubusercontent.com/fgnt/mnist/master/train-images-idx3-ubyte.gz\n",
      "Done.\n",
      "Downloading https://raw.githubusercontent.com/fgnt/mnist/master/train-labels-idx1-ubyte.gz\n",
      "Done.\n",
      "Downloading done...\n",
      "test sucessful\n"
     ]
    }
   ],
   "source": [
    "test_softmax_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "717203a1-df37-43e5-bd61-cd8b76ac495d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_one_hot_y( y, nbr_of_classes):\n",
    "    '''\n",
    "        generates a matrix of one-hot row vectors using y[i]\n",
    "        y: np.ndarray[np.uint8]\n",
    "        nbr_of_classes: int, indicating the nbr of classes.\n",
    "        all elements of y < nbr_of_classes\n",
    "\n",
    "        returns: a 2D np.ndarray[ len(y), nbr_of_classes]\n",
    "    '''\n",
    "    res = np.zeros( ( len( y), nbr_of_classes))\n",
    "    for i in range( len( y)):\n",
    "        assert( y[i] < nbr_of_classes)\n",
    "        ##  I_y: in the i-th row, set the y[i]-th column to 1\n",
    "        res[i, y[i]] = 1.0\n",
    "    return res\n",
    "\n",
    "def normalize_Z( Z):\n",
    "    '''\n",
    "        turns Z into a normalized matrix, i.e. sums over rows == 1.0 and col values corresponding to some probability\n",
    "\n",
    "        Z: np.ndarray[np.float32] 2D array\n",
    "\n",
    "        returns normalized( Z)\n",
    "    '''\n",
    "    sum_rows_Z = np.sum( Z, axis=1)  ## sum over rows\n",
    "    \n",
    "    Z_norm = np.zeros( Z.shape)  ## allocate S, the normalized Z\n",
    "\n",
    "    for i in range( Z.shape[0]):  ## for all rows\n",
    "            row_Z = Z[i, :]  ## alias for i-th row of Z\n",
    "            Z_norm[i] = row_Z/sum_rows_Z[i]  ## normalize things by dividing each entry by the sum\n",
    "\n",
    "            ## make sure we have a prob distribution...\n",
    "            assert( math.isclose( np.sum( Z_norm[i]), 1.0, rel_tol=1E-6))\n",
    "\n",
    "    return Z_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "11509f85-0b81-4bf2-8a0d-c07baf37319b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_regression_epoch(X, y, theta, lr = 0.1, batch_size=100):\n",
    "    \"\"\" Run a single epoch of SGD for softmax regression on the data, using\n",
    "    the step size lr and specified batch size.  This function should modify the\n",
    "    theta matrix in place, and you should iterate through batches in X _without_\n",
    "    randomizing the order.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray[np.float32]): 2D input array of size\n",
    "            (num_examples x input_dim).\n",
    "        y (np.ndarray[np.uint8]): 1D class label array of size (num_examples,)\n",
    "        theta (np.ndarrray[np.float32]): 2D array of softmax regression\n",
    "            parameters, of shape (input_dim, num_classes)\n",
    "        lr (float): step size (learning rate) for SGD\n",
    "        batch_size (int): size of SGD minibatch\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    ### BEGIN YOUR CODE\n",
    "    \n",
    "    def softmax_regression_batch( X, y, theta, lr, batch_size):\n",
    "        \n",
    "        ''' \n",
    "            process a single batch contained in X, y \n",
    "            \n",
    "            Args:\n",
    "                X (np.ndarray[np.float32]): 2D input array of size\n",
    "                    (batch_size x input_dim).\n",
    "                y (np.ndarray[np.uint8]): 1D class label array of size (batch_size)\n",
    "                theta (np.ndarrray[np.float32]): 2D array of softmax regression\n",
    "                    parameters, of shape (input_dim, num_classes)\n",
    "                lr (float): step size (learning rate) for SGD\n",
    "                batch_size (int): size of SGD minibatch\n",
    "\n",
    "            Returns:\n",
    "                None\n",
    "        '''\n",
    "    \n",
    "        assert( batch_size == np.shape(X)[0])\n",
    "        assert( batch_size == len( y))\n",
    "    \n",
    "        Z = np.matmul(X, theta)  ## matrix multiply\n",
    "        Z = np.exp( Z) ## exponentiate\n",
    "        sum_rows_Z = np.sum( Z, axis=1)  ## sum over rows\n",
    "    \n",
    "        Z_norm = normalize_Z( Z)  ## np.zeros( np.shape(Z))\n",
    "        I_y = gen_one_hot_y( y, np.shape( Z)[1])  ## np.zeros( np.shape(Z))\n",
    "\n",
    "        '''\n",
    "        ## compute normalized Z and I_y\n",
    "        ## YUCK another python loop\n",
    "        for i in range( len( y)):\n",
    "            row_Z = Z[i, :]  ## alias for i-th row of Z\n",
    "            Z_norm[i] = row_Z/sum_rows_Z[i]  ## normalize things by dividing each entry by the sum\n",
    "\n",
    "            ## make sure we have a prob distribution...\n",
    "            assert( math.isclose( np.sum( Z_norm[i]), 1.0, rel_tol=1E-6))\n",
    "\n",
    "            ## lastly, fill in I_y: in the i-th row, set the y[i]-th column to 1\n",
    "            ## I_y[i, y[i]] = 1.0   \n",
    "\n",
    "        ## print( \"done normalization & I_y\")\n",
    "        '''\n",
    "\n",
    "        Z_minus_I = Z_norm - I_y\n",
    "\n",
    "        grad = np.matmul( np.transpose( X), Z_minus_I)    \n",
    "\n",
    "        ## grad = grad/batch_size\n",
    "\n",
    "        assert( np.shape( grad) == np.shape( theta))\n",
    "\n",
    "        ## not equivalent with theta = theta - lr*grad... which *is not* in-place\n",
    "        theta -= (lr/batch_size) * grad\n",
    "\n",
    "        return None\n",
    "    \n",
    "    ## iterate over samples, batch by batch...\n",
    "    nbr_batches = math.ceil( len( y) / batch_size)\n",
    "    print( f'nbr_samples = {len(y)}, batch-size = {batch_size}, nbr batches = {nbr_batches}')\n",
    "    for i in range( nbr_batches):\n",
    "        lb = i * batch_size\n",
    "        ub = lb + batch_size\n",
    "        \n",
    "        ## if the nbr of samples is not an integer multiple of batch-size, we just stop\n",
    "        if ( ub > len( y)):\n",
    "            print( \"truncated samples, last batch would exceed nbr of samples\")\n",
    "            break\n",
    "            \n",
    "        X_i = X[lb:ub, :]\n",
    "        y_i = y[lb:ub]\n",
    "        softmax_regression_batch( X_i, y_i, theta, lr, batch_size)\n",
    "    \n",
    "    return None\n",
    "    \n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "4ebc77b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_softmax_regression_epoch():\n",
    "    # test numeical gradient\n",
    "    np.random.seed(0)\n",
    "    X = np.random.randn(50,5).astype(np.float32)\n",
    "    y = np.random.randint(3, size=(50,)).astype(np.uint8)\n",
    "    Theta = np.zeros((5,3), dtype=np.float32)\n",
    "    dTheta = -nd.Gradient(lambda Th : softmax_loss(X@Th.reshape(5,3),y))(Theta)\n",
    "    print( dTheta)\n",
    "    softmax_regression_epoch(X,y,Theta,lr=1.0,batch_size=50)\n",
    "    np.testing.assert_allclose(dTheta.reshape(5,3), Theta, rtol=1e-4, atol=1e-4)\n",
    "    \n",
    "    # test multi-steps on MNIST\n",
    "    X,y = parse_mnist(\"train-images-idx3-ubyte.gz\",\n",
    "                      \"train-labels-idx1-ubyte.gz\")\n",
    "    theta = np.zeros((X.shape[1], y.max()+1), dtype=np.float32)\n",
    "    softmax_regression_epoch(X[:100], y[:100], theta, lr=0.1, batch_size=10)\n",
    "    np.testing.assert_allclose(np.linalg.norm(theta), 1.0947356, \n",
    "                               rtol=1e-5, atol=1e-5)\n",
    "    \n",
    "    print( \"test done...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "15bc0f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02525946 -0.11465225  0.13991171  0.02291308 -0.08477389  0.06186081\n",
      "  0.04386758  0.00742978 -0.05129736 -0.07611353  0.0494122   0.02670133\n",
      "  0.01691807  0.01340648 -0.03032455]\n",
      "nbr_samples = 50, batch-size = 50, nbr batches = 1\n",
      "Downloading train data: https://raw.githubusercontent.com/fgnt/mnist/master/train-images-idx3-ubyte.gz, https://raw.githubusercontent.com/fgnt/mnist/master/train-labels-idx1-ubyte.gz\n",
      "Downloading https://raw.githubusercontent.com/fgnt/mnist/master/train-images-idx3-ubyte.gz\n",
      "Done.\n",
      "Downloading https://raw.githubusercontent.com/fgnt/mnist/master/train-labels-idx1-ubyte.gz\n",
      "Done.\n",
      "Downloading done...\n",
      "nbr_samples = 100, batch-size = 10, nbr batches = 10\n",
      "test done...\n"
     ]
    }
   ],
   "source": [
    "test_softmax_regression_epoch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "2d4b1a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_err(h,y):\n",
    "    \"\"\" Helper function to compute both loss and error\"\"\"\n",
    "    return softmax_loss(h,y), np.mean(h.argmax(axis=1) != y)\n",
    "\n",
    "\n",
    "def train_softmax(X_tr, y_tr, X_te, y_te, epochs=10, lr=0.5, batch=100,\n",
    "                  cpp=False):\n",
    "    \"\"\" Example function to fully train a softmax regression classifier \"\"\"\n",
    "    theta = np.zeros((X_tr.shape[1], y_tr.max()+1), dtype=np.float32)\n",
    "    print(\"| Epoch | Train Loss | Train Err | Test Loss | Test Err |\")\n",
    "    for epoch in range(epochs):\n",
    "        if not cpp:\n",
    "            softmax_regression_epoch(X_tr, y_tr, theta, lr=lr, batch_size=batch)\n",
    "        else:\n",
    "            softmax_regression_epoch_cpp(X_tr, y_tr, theta, lr=lr, batch=batch)\n",
    "        train_loss, train_err = loss_err(X_tr @ theta, y_tr)\n",
    "        test_loss, test_err = loss_err(X_te @ theta, y_te)\n",
    "        print(\"|  {:>4} |    {:.5f} |   {:.5f} |   {:.5f} |  {:.5f} |\"\\\n",
    "              .format(epoch, train_loss, train_err, test_loss, test_err))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "d6fa66d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train data: https://raw.githubusercontent.com/fgnt/mnist/master/train-images-idx3-ubyte.gz, https://raw.githubusercontent.com/fgnt/mnist/master/train-labels-idx1-ubyte.gz\n",
      "Downloading https://raw.githubusercontent.com/fgnt/mnist/master/train-images-idx3-ubyte.gz\n",
      "Done.\n",
      "Downloading https://raw.githubusercontent.com/fgnt/mnist/master/train-labels-idx1-ubyte.gz\n",
      "Done.\n",
      "Downloading done...\n",
      "Downloading train data: https://raw.githubusercontent.com/fgnt/mnist/master/t10k-images-idx3-ubyte.gz, https://raw.githubusercontent.com/fgnt/mnist/master/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://raw.githubusercontent.com/fgnt/mnist/master/t10k-images-idx3-ubyte.gz\n",
      "Done.\n",
      "Downloading https://raw.githubusercontent.com/fgnt/mnist/master/t10k-labels-idx1-ubyte.gz\n",
      "Done.\n",
      "Downloading done...\n",
      "Training softmax regression\n",
      "| Epoch | Train Loss | Train Err | Test Loss | Test Err |\n",
      "nbr_samples = 60000, batch-size = 100, nbr batches = 600\n",
      "|     0 |    0.38625 |   0.10812 |   0.36690 |  0.09960 |\n",
      "nbr_samples = 60000, batch-size = 100, nbr batches = 600\n",
      "|     1 |    0.34486 |   0.09748 |   0.32926 |  0.09180 |\n",
      "nbr_samples = 60000, batch-size = 100, nbr batches = 600\n",
      "|     2 |    0.32663 |   0.09187 |   0.31376 |  0.08770 |\n",
      "nbr_samples = 60000, batch-size = 100, nbr batches = 600\n",
      "|     3 |    0.31572 |   0.08867 |   0.30504 |  0.08510 |\n",
      "nbr_samples = 60000, batch-size = 100, nbr batches = 600\n",
      "|     4 |    0.30822 |   0.08667 |   0.29940 |  0.08320 |\n",
      "nbr_samples = 60000, batch-size = 100, nbr batches = 600\n",
      "|     5 |    0.30264 |   0.08508 |   0.29543 |  0.08250 |\n",
      "nbr_samples = 60000, batch-size = 100, nbr batches = 600\n",
      "|     6 |    0.29825 |   0.08393 |   0.29247 |  0.08180 |\n",
      "nbr_samples = 60000, batch-size = 100, nbr batches = 600\n",
      "|     7 |    0.29466 |   0.08305 |   0.29017 |  0.08120 |\n",
      "nbr_samples = 60000, batch-size = 100, nbr batches = 600\n",
      "|     8 |    0.29166 |   0.08215 |   0.28832 |  0.08070 |\n",
      "nbr_samples = 60000, batch-size = 100, nbr batches = 600\n",
      "|     9 |    0.28908 |   0.08137 |   0.28680 |  0.08080 |\n"
     ]
    }
   ],
   "source": [
    "X_tr, y_tr = parse_mnist(\"train-images-idx3-ubyte.gz\",\n",
    "                         \"train-labels-idx1-ubyte.gz\")\n",
    "X_te, y_te = parse_mnist(\"t10k-images-idx3-ubyte.gz\",\n",
    "                         \"t10k-labels-idx1-ubyte.gz\",\n",
    "                        num_samples = 10000)\n",
    "\n",
    "print(\"Training softmax regression\")\n",
    "train_softmax(X_tr, y_tr, X_te, y_te, epochs=10, lr = 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "29b0e77b-2aa1-44a7-957a-aa04751518b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU( m):\n",
    "    '''\n",
    "    returns ReLU (rectified linear unit) of 2D array m as a copy\n",
    "    0 for values x < 0, x if x >= 0 \n",
    "    '''\n",
    "    return( np.maximum(0, m))\n",
    "\n",
    "def ReLU_derivative( m):\n",
    "    '''\n",
    "    return derivative of ReLU of 2D array m as a copy\n",
    "    note: the derivative is 0 when x = 0 and 1 when x > 0\n",
    "    '''\n",
    "    return (m > 0) * 1  ## one of those python tricks... turn booleans into values\n",
    "    \n",
    "def nn_epoch(X, y, W_1, W_2, lr = 0.1, batch_size=100):\n",
    "    \"\"\" Run a single epoch of SGD for a two-layer neural network defined by the\n",
    "    weights W1 and W2 (with no bias terms):\n",
    "        logits = ReLU(X * W1) * W2\n",
    "    The function should use the step size lr, and the specified batch size (and\n",
    "    again, without randomizing the order of X).  It should modify the\n",
    "    W1 and W2 matrices in place.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray[np.float32]): 2D input array of size\n",
    "            (num_examples x input_dim).\n",
    "        y (np.ndarray[np.uint8]): 1D class label array of size (num_examples,)\n",
    "        W1 (np.ndarray[np.float32]): 2D array of first layer weights, of shape\n",
    "            (input_dim, hidden_dim)\n",
    "        W2 (np.ndarray[np.float32]): 2D array of second layer weights, of shape\n",
    "            (hidden_dim, num_classes)\n",
    "        lr (float): step size (learning rate) for SGD\n",
    "        batch_size (int): size of SGD minibatch\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    def nn_batch( X, y, W_1, W_2, lr = 0.1, batch_size=100):\n",
    "        \n",
    "        assert( X.shape[1] == W_1.shape[0])\n",
    "        assert( X.shape[0] == batch_size)\n",
    "        assert( len( y) == batch_size)\n",
    "        \n",
    "        num_examples = X.shape[0]  ## nbr of rows in batch\n",
    "        input_dim = X.shape[1]     ## size of input vectors\n",
    "        hidden_dim = W_1.shape[1]   ## size of hidden layer\n",
    "        num_classes = W_2.shape[1]  ## size of output\n",
    "    \n",
    "        X_W1 = np.matmul( X, W_1)\n",
    "        Z_1 = ReLU( X_W1)\n",
    "        assert( Z_1.shape == ( num_examples, hidden_dim))\n",
    "    \n",
    "        ## multiply Z_1, W_2, exponentiate result and normalize\n",
    "        Z_1_W_2 = normalize_Z( np.exp( np.matmul( Z_1, W_2)))\n",
    "        I_y = gen_one_hot_y( y, num_classes)\n",
    "        assert( I_y.shape == Z_1_W_2.shape)\n",
    "    \n",
    "        G_2 = Z_1_W_2 - I_y  \n",
    "        ## elementwise multiply of the ReLU_derivative of Z_1 with (G_2 * W_2^T)\n",
    "        G_1 = np.multiply( ReLU_derivative( Z_1), np.matmul( G_2, np.transpose( W_2)))\n",
    "    \n",
    "        assert( G_2.shape == ( num_examples, num_classes))\n",
    "        assert( G_1.shape == ( num_examples, hidden_dim))\n",
    "\n",
    "        grad_W_1 = np.matmul( np.transpose( X), G_1)/batch_size\n",
    "        grad_W_2 = np.matmul( np.transpose( Z_1), G_2)/batch_size\n",
    "\n",
    "        W_1 -= lr * grad_W_1\n",
    "        W_2 -= lr * grad_W_2\n",
    "    \n",
    "    ## iterate over samples, batch by batch...\n",
    "    nbr_batches = math.ceil( len( y) / batch_size)\n",
    "    print( f'nbr_samples = {len(y)}, batch-size = {batch_size}, nbr batches = {nbr_batches}')\n",
    "    for i in range( nbr_batches):\n",
    "        lb = i * batch_size\n",
    "        ub = lb + batch_size\n",
    "        \n",
    "        ## if the nbr of samples is not an integer multiple of batch-size, we just stop\n",
    "        if ( ub > len( y)):\n",
    "            print( \"truncated samples, last batch would exceed nbr of samples\")\n",
    "            break\n",
    "            \n",
    "        X_i = X[lb:ub, :]\n",
    "        y_i = y[lb:ub]\n",
    "        nn_batch( X_i, y_i, W_1, W_2, lr, batch_size)\n",
    "    \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ca4a9ff5-9166-4e24-8c14-61d420e1dd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(X_tr, y_tr, X_te, y_te, hidden_dim = 500,\n",
    "             epochs=10, lr=0.5, batch_size=100):\n",
    "    \"\"\" Example function to train two layer neural network \"\"\"\n",
    "    n, k = X_tr.shape[1], y_tr.max() + 1\n",
    "    np.random.seed(0)\n",
    "    W1 = np.random.randn(n, hidden_dim).astype(np.float32) / np.sqrt(hidden_dim)\n",
    "    W2 = np.random.randn(hidden_dim, k).astype(np.float32) / np.sqrt(k)\n",
    "\n",
    "    print(\"| Epoch | Train Loss | Train Err | Test Loss | Test Err |\")\n",
    "    for epoch in range(epochs):\n",
    "        nn_epoch(X_tr, y_tr, W1, W2, lr=lr, batch_size=batch_size)\n",
    "        train_loss, train_err = loss_err(np.maximum(X_tr@W1,0)@W2, y_tr)\n",
    "        test_loss, test_err = loss_err(np.maximum(X_te@W1,0)@W2, y_te)\n",
    "        print(\"|  {:>4} |    {:.5f} |   {:.5f} |   {:.5f} |  {:.5f} |\"\\\n",
    "              .format(epoch, train_loss, train_err, test_loss, test_err))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "35c03575-dc8e-4889-94bf-c6835ca8b08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_nn_epoch():\n",
    "\n",
    "    # test nn gradients\n",
    "    np.random.seed(0)\n",
    "    X = np.random.randn(50,5).astype(np.float32)\n",
    "    y = np.random.randint(3, size=(50,)).astype(np.uint8)\n",
    "    W1 = np.random.randn(5, 10).astype(np.float32) / np.sqrt(10)\n",
    "    W2 = np.random.randn(10, 3).astype(np.float32) / np.sqrt(3)\n",
    "    dW1 = nd.Gradient(lambda W1_ : \n",
    "        softmax_loss(np.maximum(X@W1_.reshape(5,10),0)@W2, y))(W1)\n",
    "    dW2 = nd.Gradient(lambda W2_ : \n",
    "        softmax_loss(np.maximum(X@W1,0)@W2_.reshape(10,3), y))(W2)\n",
    "    W1_0, W2_0 = W1.copy(), W2.copy()\n",
    "\n",
    "    nn_epoch(X, y, W1, W2, lr=1.0, batch_size=50)\n",
    "    print( \"done first nn_epoch...\")\n",
    "    \n",
    "    np.testing.assert_allclose(dW1.reshape(5,10), W1_0-W1, rtol=1e-4, atol=1e-4)\n",
    "    np.testing.assert_allclose(dW2.reshape(10,3), W2_0-W2, rtol=1e-4, atol=1e-4)\n",
    "\n",
    "    # test full epoch\n",
    "    X,y = parse_mnist(\"train-images-idx3-ubyte.gz\",\n",
    "                      \"train-labels-idx1-ubyte.gz\")\n",
    "    np.random.seed(0)\n",
    "    W1 = np.random.randn(X.shape[1], 100).astype(np.float32) / np.sqrt(100)\n",
    "    W2 = np.random.randn(100, 10).astype(np.float32) / np.sqrt(10)\n",
    "    nn_epoch(X, y, W1, W2, lr=0.2, batch_size=100)\n",
    "    print( \"done second nn_epoch...\")\n",
    "    np.testing.assert_allclose(np.linalg.norm(W1), 28.437788, \n",
    "                               rtol=1e-5, atol=1e-5)\n",
    "    np.testing.assert_allclose(np.linalg.norm(W2), 10.455095, \n",
    "                               rtol=1e-5, atol=1e-5)\n",
    "    np.testing.assert_allclose(loss_err(np.maximum(X@W1,0)@W2, y),\n",
    "                               (0.19770025, 0.06006667), rtol=1e-4, atol=1e-4)\n",
    "\n",
    "    print( \"test ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d1de931b-64f8-4857-b844-e0efac8acbc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nbr_samples = 50, batch-size = 50, nbr batches = 1\n",
      "done first nn_epoch...\n",
      "Downloading train data: https://raw.githubusercontent.com/fgnt/mnist/master/train-images-idx3-ubyte.gz, https://raw.githubusercontent.com/fgnt/mnist/master/train-labels-idx1-ubyte.gz\n",
      "Downloading https://raw.githubusercontent.com/fgnt/mnist/master/train-images-idx3-ubyte.gz\n",
      "Done.\n",
      "Downloading https://raw.githubusercontent.com/fgnt/mnist/master/train-labels-idx1-ubyte.gz\n",
      "Done.\n",
      "Downloading done...\n",
      "nbr_samples = 60000, batch-size = 100, nbr batches = 600\n",
      "done second nn_epoch...\n",
      "test ok\n"
     ]
    }
   ],
   "source": [
    "test_nn_epoch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "8308f34b-1a51-4044-be33-6f9bbc0b645a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train data: https://raw.githubusercontent.com/fgnt/mnist/master/train-images-idx3-ubyte.gz, https://raw.githubusercontent.com/fgnt/mnist/master/train-labels-idx1-ubyte.gz\n",
      "Downloading https://raw.githubusercontent.com/fgnt/mnist/master/train-images-idx3-ubyte.gz\n",
      "Done.\n",
      "Downloading https://raw.githubusercontent.com/fgnt/mnist/master/train-labels-idx1-ubyte.gz\n",
      "Done.\n",
      "Downloading done...\n",
      "Downloading train data: https://raw.githubusercontent.com/fgnt/mnist/master/t10k-images-idx3-ubyte.gz, https://raw.githubusercontent.com/fgnt/mnist/master/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://raw.githubusercontent.com/fgnt/mnist/master/t10k-images-idx3-ubyte.gz\n",
      "Done.\n",
      "Downloading https://raw.githubusercontent.com/fgnt/mnist/master/t10k-labels-idx1-ubyte.gz\n",
      "Done.\n",
      "Downloading done...\n",
      "Training 2 layer neural network\n",
      "| Epoch | Train Loss | Train Err | Test Loss | Test Err |\n",
      "nbr_samples = 60000, batch-size = 100, nbr batches = 600\n",
      "|     0 |    0.15143 |   0.04567 |   0.16062 |  0.04830 |\n",
      "nbr_samples = 60000, batch-size = 100, nbr batches = 600\n",
      "|     1 |    0.10331 |   0.03058 |   0.12111 |  0.03740 |\n",
      "nbr_samples = 60000, batch-size = 100, nbr batches = 600\n",
      "|     2 |    0.07898 |   0.02263 |   0.10352 |  0.03180 |\n",
      "nbr_samples = 60000, batch-size = 100, nbr batches = 600\n",
      "|     3 |    0.06314 |   0.01735 |   0.09256 |  0.02840 |\n",
      "nbr_samples = 60000, batch-size = 100, nbr batches = 600\n",
      "|     4 |    0.05252 |   0.01397 |   0.08568 |  0.02600 |\n",
      "nbr_samples = 60000, batch-size = 100, nbr batches = 600\n",
      "|     5 |    0.04454 |   0.01142 |   0.08068 |  0.02480 |\n",
      "nbr_samples = 60000, batch-size = 100, nbr batches = 600\n",
      "|     6 |    0.03857 |   0.00935 |   0.07728 |  0.02430 |\n",
      "nbr_samples = 60000, batch-size = 100, nbr batches = 600\n",
      "|     7 |    0.03399 |   0.00800 |   0.07476 |  0.02410 |\n",
      "nbr_samples = 60000, batch-size = 100, nbr batches = 600\n",
      "|     8 |    0.03006 |   0.00653 |   0.07239 |  0.02380 |\n",
      "nbr_samples = 60000, batch-size = 100, nbr batches = 600\n",
      "|     9 |    0.02714 |   0.00580 |   0.07118 |  0.02320 |\n"
     ]
    }
   ],
   "source": [
    "X_tr, y_tr = parse_mnist(\"train-images-idx3-ubyte.gz\",\n",
    "                         \"train-labels-idx1-ubyte.gz\")\n",
    "X_te, y_te = parse_mnist(\"t10k-images-idx3-ubyte.gz\",\n",
    "                         \"t10k-labels-idx1-ubyte.gz\",\n",
    "                        num_samples = 10000)\n",
    "\n",
    "print(\"Training 2 layer neural network\")\n",
    "train_nn(X_tr, y_tr, X_te, y_te, epochs=10, lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83938629-0c23-4cad-a58d-b5a28de47123",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
