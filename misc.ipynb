{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c5ae26c-bc3a-4132-95d4-8796050b1940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of f(X) = LogSumExp(X) + max(X):\n",
      " [[[0.02421295 0.06581762 1.17891085]\n",
      "  [0.06581762 0.17891085 1.48633011]]\n",
      "\n",
      " [[0.02421295 0.06581762 1.17891085]\n",
      "  [0.06581762 0.17891085 1.48633011]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def logsumexp(X, axis=None, keepdims=True):\n",
    "    \"\"\"Compute LogSumExp along specified axes with numerical stability.\"\"\"\n",
    "    X_max = np.max(X, axis=axis, keepdims=True)  # Max for numerical stability\n",
    "    print( X_max)\n",
    "    x_minus_x_max = X - X_max\n",
    "    print( x_minus_x_max)\n",
    "    exp_x = np.exp( x_minus_x_max)\n",
    "    print( exp_x)\n",
    "    sum_x = np.sum( exp_x, axis=axis, keepdims=True)\n",
    "    print( sum_x)\n",
    "    res = np.log( sum_x) + X_max\n",
    "    return res\n",
    "    \n",
    "    return np.log(np.sum(np.exp(X - X_max), axis=axis, keepdims=True)) + X_max\n",
    "\n",
    "def gradient_logsumexp(X, axis=None, keepdims=True):\n",
    "    \"\"\"Compute gradient of LogSumExp along specified axes.\"\"\"\n",
    "    X_max = np.max(X, axis=axis, keepdims=True)\n",
    "    exp_X = np.exp(X - X_max)\n",
    "    softmax = exp_X / np.sum(exp_X, axis=axis, keepdims=True)  # Softmax over axis\n",
    "    return softmax  # Gradient of LogSumExp\n",
    "\n",
    "def gradient_max(X, axis=None, keepdims=True):\n",
    "    \"\"\"Compute gradient of max(X) along specified axes.\"\"\"\n",
    "    X_max = np.max(X, axis=axis, keepdims=True)\n",
    "    mask = (X == X_max).astype(float)  # Indicator function\n",
    "    mask /= np.sum(mask, axis=axis, keepdims=True)  # Normalize when multiple maxima exist\n",
    "    return mask  # Gradient of max(X)\n",
    "\n",
    "def gradient_f(X, axis_lse=None, axis_max=None, keepdims=True):\n",
    "    \"\"\"Compute gradient of f(X) = logsumexp(X) + max(X).\"\"\"\n",
    "    grad_lse = gradient_logsumexp(X, axis=axis_lse, keepdims=keepdims)\n",
    "    grad_max = gradient_max(X, axis=axis_max, keepdims=keepdims)\n",
    "    return grad_lse + grad_max  # Sum of both gradients\n",
    "\n",
    "# Example Usage\n",
    "X = np.array([\n",
    "    [[1.0, 2.0, 3.0], [2.0, 3.0, 4.0]],\n",
    "    [[0.5, 1.5, 2.5], [1.5, 2.5, 3.5]]\n",
    "])  # Shape: (2, 2, 3)\n",
    "\n",
    "axis_lse = (1, 2)  # Compute LogSumExp over last two axes\n",
    "axis_max = (2,)    # Compute max over last axis\n",
    "\n",
    "grad_output = gradient_f(X, axis_lse=axis_lse, axis_max=axis_max)\n",
    "print(\"Gradient of f(X) = LogSumExp(X) + max(X):\\n\", grad_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a839de3-256c-4ec4-9c3a-ab6b5b8dd754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2]\n",
      " [5]\n",
      " [8]]\n",
      "[[-2 -1  0]\n",
      " [-2 -1  0]\n",
      " [-2 -1  0]]\n",
      "[[0.13533528 0.36787944 1.        ]\n",
      " [0.13533528 0.36787944 1.        ]\n",
      " [0.13533528 0.36787944 1.        ]]\n",
      "[[1.50321472]\n",
      " [1.50321472]\n",
      " [1.50321472]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.40760596],\n",
       "       [5.40760596],\n",
       "       [8.40760596]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([\n",
    "     [0, 1, 2], \n",
    "     [3, 4, 5],\n",
    "     [6, 7, 8]\n",
    "  ]) \n",
    "logsumexp( X, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30871a74-9939-4da1-9365-f6dd355d05d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(X):\n",
    "    \"\"\"Compute LogSoftmax row-wise for a 2D matrix X.\"\"\"\n",
    "    X_max = np.max(X, axis=1, keepdims=True)  # For numerical stability\n",
    "    exp_X = np.exp(X - X_max)\n",
    "    softmax_X = exp_X / np.sum(exp_X, axis=1, keepdims=True)\n",
    "    return np.log(softmax_X)\n",
    "\n",
    "def log_softmax_gradient(X):\n",
    "    \"\"\"Compute the gradient of LogSoftmax for a 2D matrix X.\"\"\"\n",
    "    softmax_X = np.exp(X - np.max(X, axis=1, keepdims=True))  # Compute softmax\n",
    "    softmax_X /= np.sum(softmax_X, axis=1, keepdims=True)\n",
    "\n",
    "    batch_size, num_classes = X.shape\n",
    "    I = np.eye(num_classes)  # Identity matrix of size (num_classes x num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2263ad43-be23-4011-a6fa-856af9f61be4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.40760596, -1.40760596, -0.40760596],\n",
       "       [-2.40760596, -1.40760596, -0.40760596],\n",
       "       [-2.40760596, -1.40760596, -0.40760596]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_softmax( X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eecae11-dd3a-4857-9504-970802911173",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
